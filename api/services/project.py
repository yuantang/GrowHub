# -*- coding: utf-8 -*-
# GrowHub Project Service - ç›‘æ§é¡¹ç›®ç®¡ç†æœåŠ¡
# ç»Ÿä¸€ç®¡ç†å…³é”®è¯ã€è°ƒåº¦å’Œé€šçŸ¥

import asyncio
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any
from pydantic import BaseModel
from ..services.account_pool import AccountStatus


class ProjectConfig(BaseModel):
    """é¡¹ç›®é…ç½®æ¨¡å‹"""
    name: str
    description: Optional[str] = None
    
    # å…³é”®è¯
    keywords: List[str] = []
    # èˆ†æƒ…è¯
    sentiment_keywords: List[str] = []
    
    # å¹³å°
    platforms: List[str] = ["xhs"]
    
    # çˆ¬è™«é…ç½®
    crawler_type: str = "search"
    crawl_limit: int = 20
    crawl_date_range: int = 7
    enable_comments: bool = True
    deduplicate_authors: bool = False
    max_concurrency: int = 3  # æœ€å¤§å¹¶å‘æ•° (Pro ç‰ˆç‰¹æ€§)
    
    # é«˜çº§è¿‡æ»¤å™¨
    min_likes: int = 0
    max_likes: int = 0
    min_comments: int = 0
    max_comments: int = 0
    min_shares: int = 0
    max_shares: int = 0
    min_favorites: int = 0
    max_favorites: int = 0
    
    # è°ƒåº¦é…ç½®
    schedule_type: str = "interval"  # interval / cron
    schedule_value: str = "3600"     # é»˜è®¤1å°æ—¶
    is_active: bool = False
    
    # é€šçŸ¥é…ç½®
    alert_on_negative: bool = True
    alert_on_hotspot: bool = False
    alert_channels: List[str] = []


class ProjectInfo(BaseModel):
    """é¡¹ç›®ä¿¡æ¯ï¼ˆåŒ…å«è¿è¡ŒçŠ¶æ€ï¼‰"""
    id: int
    name: str
    description: Optional[str]
    keywords: List[str]
    sentiment_keywords: List[str] = []
    platforms: List[str]
    crawler_type: str
    crawl_limit: int
    enable_comments: bool
    deduplicate_authors: bool
    max_concurrency: int
    schedule_type: str
    schedule_value: str
    is_active: bool
    alert_on_negative: bool
    alert_on_hotspot: bool
    alert_channels: List[str]
    
    # è¿è¡ŒçŠ¶æ€
    last_run_at: Optional[datetime]
    next_run_at: Optional[datetime]
    run_count: int
    
    # ç»Ÿè®¡
    total_crawled: int
    total_alerts: int
    today_crawled: int
    today_alerts: int
    
    created_at: datetime
    updated_at: datetime


class ProjectService:
    """ç›‘æ§é¡¹ç›®æœåŠ¡"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    async def get_project_logs(self, project_id: int, limit: int = 100) -> List[str]:
        """è·å–é¡¹ç›®è¿è¡Œæ—¥å¿—"""
        return self._project_logs.get(project_id, [])[-limit:]

    def append_log(self, project_id: int, message: str):
        """æ·»åŠ æ—¥å¿—"""
        if project_id not in self._project_logs:
            self._project_logs[project_id] = []
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self._project_logs[project_id].append(log_entry)
        # ä¿æŒæœ€æ–°çš„ 1000 æ¡
        if len(self._project_logs[project_id]) > 1000:
            self._project_logs[project_id] = self._project_logs[project_id][-1000:]
        print(f"[Project-{project_id}] {message}")  # ä¿ç•™æ§åˆ¶å°è¾“å‡º

    def __init__(self):
        if self._initialized:
            return
        self._initialized = True
        self._project_logs: Dict[int, List[str]] = {}
    
    async def sync_active_projects_to_scheduler(self):
        """Startup sync: Register all active projects with scheduler (after server restart)"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.is_active == True)
            )
            active_projects = result.scalars().all()
            
            registered_count = 0
            for project in active_projects:
                try:
                    await self._register_scheduler_task(project)
                    registered_count += 1
                except Exception as e:
                    print(f"[Scheduler Sync] Failed to register project {project.id}: {e}")
            
            print(f"[Scheduler Sync] Registered {registered_count}/{len(active_projects)} active projects")
    
    async def create_project(self, config: ProjectConfig) -> Dict[str, Any]:
        """åˆ›å»ºç›‘æ§é¡¹ç›®"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        
        async with get_session() as session:
            project = GrowHubProject(
                name=config.name,
                description=config.description,
                keywords=config.keywords,
                sentiment_keywords=config.sentiment_keywords,
                platforms=config.platforms,
                crawler_type=config.crawler_type,
                crawl_limit=config.crawl_limit,
                crawl_date_range=config.crawl_date_range,
                enable_comments=config.enable_comments,
                deduplicate_authors=config.deduplicate_authors,
                min_likes=config.min_likes,
                max_likes=config.max_likes,
                min_comments=config.min_comments,
                max_comments=config.max_comments,
                min_shares=config.min_shares,
                max_shares=config.max_shares,
                min_favorites=config.min_favorites,
                max_favorites=config.max_favorites,
                schedule_type=config.schedule_type,
                schedule_value=config.schedule_value,
                is_active=False,  # åˆ›å»ºæ—¶é»˜è®¤ä¸å¯åŠ¨
                alert_on_negative=config.alert_on_negative,
                alert_on_hotspot=config.alert_on_hotspot,
                alert_channels=config.alert_channels,
            )
            session.add(project)
            await session.flush()
            await session.refresh(project)
            
            project_id = project.id
            
            # å¦‚æœéœ€è¦ç«‹å³å¯åŠ¨
            if config.is_active:
                await self._register_scheduler_task(project)
                project.is_active = True
            
            return {
                "id": project_id,
                "name": config.name,
                "message": "é¡¹ç›®åˆ›å»ºæˆåŠŸ"
            }
    
    async def get_project(self, project_id: int) -> Optional[Dict[str, Any]]:
        """è·å–é¡¹ç›®è¯¦æƒ…"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            if not project:
                return None
            
            # Fetch latest checkpoint info
            from checkpoint.manager import get_checkpoint_manager
            from database.growhub_models import GrowHubCheckpoint
            from sqlalchemy import desc
            
            cp_result = await session.execute(
                select(GrowHubCheckpoint)
                .where(GrowHubCheckpoint.project_id == project_id)
                .order_by(desc(GrowHubCheckpoint.updated_at))
                .limit(1)
            )
            latest_cp = cp_result.scalar()
            
            project_dict = self._to_dict(project)
            if latest_cp:
                project_dict["latest_checkpoint"] = {
                    "task_id": latest_cp.id,
                    "status": latest_cp.status.value if hasattr(latest_cp.status, 'value') else latest_cp.status,
                    "total_notes": latest_cp.total_notes_fetched,
                    "total_comments": latest_cp.total_comments_fetched,
                    "total_errors": latest_cp.total_errors,
                    "current_page": latest_cp.current_page,
                    "last_update": latest_cp.updated_at.isoformat() if latest_cp.updated_at else None
                }
            else:
                project_dict["latest_checkpoint"] = None
                
            return project_dict

    
    async def list_projects(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰é¡¹ç›®åˆ—è¡¨"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select, desc
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).order_by(desc(GrowHubProject.updated_at))
            )
            projects = result.scalars().all()
            
            return [self._to_dict(p) for p in projects]
    
    async def update_project(self, project_id: int, updates: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ›´æ–°é¡¹ç›®é…ç½®"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            
            if not project:
                return None
            
            # æ›´æ–°å­—æ®µ
            for key, value in updates.items():
                if hasattr(project, key) and key not in ['id', 'created_at']:
                    setattr(project, key, value)
            
            project.updated_at = datetime.now()
            
            # å¦‚æœè°ƒåº¦é…ç½®å˜æ›´ï¼Œéœ€è¦æ›´æ–°è°ƒåº¦å™¨
            schedule_changed = 'schedule_type' in updates or 'schedule_value' in updates
            active_changed = 'is_active' in updates
            
            if schedule_changed or active_changed:
                if project.is_active:
                    await self._unregister_scheduler_task(project)
                    await self._register_scheduler_task(project)
                else:
                    await self._unregister_scheduler_task(project)
            
            # Commit changes to database
            await session.commit()
            
            return self._to_dict(project)
    
    async def delete_project(self, project_id: int) -> bool:
        """åˆ é™¤é¡¹ç›®"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            
            if not project:
                return False
            
            # å…ˆå–æ¶ˆè°ƒåº¦ä»»åŠ¡
            await self._unregister_scheduler_task(project)
            
            await session.delete(project)
            return True
    
    async def start_project(self, project_id: int) -> Dict[str, Any]:
        """å¯åŠ¨é¡¹ç›®ï¼ˆå¼€å§‹è‡ªåŠ¨è°ƒåº¦ï¼‰"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            
            if not project:
                return {"success": False, "error": "é¡¹ç›®ä¸å­˜åœ¨"}
            
            if not project.keywords:
                return {"success": False, "error": "è¯·å…ˆé…ç½®å…³é”®è¯"}
            
            if not project.platforms:
                return {"success": False, "error": "è¯·å…ˆé€‰æ‹©å¹³å°"}
            
            # æ³¨å†Œè°ƒåº¦ä»»åŠ¡
            await self._register_scheduler_task(project)
            project.is_active = True
            project.updated_at = datetime.now()
            
            return {"success": True, "message": "é¡¹ç›®å·²å¯åŠ¨"}
    
    async def stop_project(self, project_id: int) -> Dict[str, Any]:
        """åœæ­¢é¡¹ç›®"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            
            if not project:
                return {"success": False, "error": "é¡¹ç›®ä¸å­˜åœ¨"}
            
            await self._unregister_scheduler_task(project)
            project.is_active = False
            project.updated_at = datetime.now()
            
            return {"success": True, "message": "é¡¹ç›®å·²åœæ­¢"}
    
    async def run_project_now(self, project_id: int) -> Dict[str, Any]:
        """ç«‹å³è¿è¡Œé¡¹ç›®ï¼ˆæ‰‹åŠ¨è§¦å‘ä¸€æ¬¡ï¼‰"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            
            if not project:
                return {"success": False, "error": "é¡¹ç›®ä¸å­˜åœ¨"}
            
            # å¼‚æ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            asyncio.create_task(self._execute_project(project_id))
            
            return {"success": True, "message": "ä»»åŠ¡å·²å¼€å§‹æ‰§è¡Œ"}
    
    async def _execute_project(self, project_id: int):
        """æ‰§è¡Œé¡¹ç›®çˆ¬è™«ä»»åŠ¡"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject
        from sqlalchemy import select
        from api.services.crawler_manager import crawler_manager
        from api.schemas import CrawlerStartRequest
        from api.services.account_pool import get_account_pool, AccountPlatform
        
        async with get_session() as session:
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            
            if not project:
                print(f"[Project] é¡¹ç›® {project_id} ä¸å­˜åœ¨")
                return
            
            project.last_run_at = datetime.now()
            project.run_count = (project.run_count or 0) + 1
            await session.commit()  # Persist run statistics immediately
            
            # æ¸…ç©ºæ—§æ—¥å¿—å¹¶å¼€å§‹è®°å½•
            self._project_logs[project_id] = []
            self.append_log(project_id, f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {project.name}")
            self.append_log(project_id, f"å…³é”®è¯: {project.keywords}")
            
            keywords_str = ",".join(project.keywords or [])
            platforms = project.platforms or ["xhs"]
            
            total_crawled_items = 0
            # start_time_utc for DB queries, start_time_local for duration logging
            start_time_utc = datetime.now(timezone.utc).replace(tzinfo=None)
            start_time_local = datetime.now()
            
            
            MAX_ACCOUNT_RETRIES = 3  # æœ€å¤§è´¦å·åˆ‡æ¢æ¬¡æ•°
            
            for platform in platforms:
                # å¹³å°åç§°æ˜ å°„
                platform_names = {
                    "xhs": "å°çº¢ä¹¦",
                    "douyin": "æŠ–éŸ³", "dy": "æŠ–éŸ³",
                    "bilibili": "Bç«™", "bili": "Bç«™",
                    "weibo": "å¾®åš", "wb": "å¾®åš",
                    "zhihu": "çŸ¥ä¹",
                    "kuaishou": "å¿«æ‰‹", "ks": "å¿«æ‰‹",
                    "tieba": "è´´å§"
                }
                display_platform = platform_names.get(platform, platform)
                
                # è´¦å·é‡è¯•å¾ªç¯
                success_this_platform = False
                tried_accounts = []
                
                for retry_num in range(MAX_ACCOUNT_RETRIES):
                    # è·å–è´¦å·ï¼ˆæ’é™¤å·²å°è¯•çš„ï¼‰
                    pool = get_account_pool()
                    try:
                        plat_enum = AccountPlatform(platform)
                        self.append_log(project_id, f"æ­£åœ¨è·å– {display_platform} å¹³å°è´¦å· (å°è¯• {retry_num + 1}/{MAX_ACCOUNT_RETRIES})...")
                        
                        # è·å–æ‰€æœ‰å¯ç”¨è´¦å·ä¸­æœªå°è¯•è¿‡çš„ (Sticky Sessions: ä¼ å…¥ project_id)
                        account = await pool.get_available_account(plat_enum, exclude_ids=tried_accounts, project_id=project_id)
                        
                        if not account and retry_num == 0:
                            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å°è¯•ä¸”æ²¡æœ‰å¯ç”¨è´¦å·ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è´¦å·å³å°†ç»“æŸå†·å´ (Wait up to 15s if an account is almost ready)
                            all_accounts = await pool.get_all_accounts(plat_enum)
                            now = datetime.now()
                            soon_available = [a for a in all_accounts if a.id not in tried_accounts and a.status == AccountStatus.ACTIVE and a.cooldown_until and now < a.cooldown_until < now + timedelta(seconds=20)]
                            
                            if soon_available:
                                next_ready = min(soon_available, key=lambda a: a.cooldown_until)
                                wait_sec = (next_ready.cooldown_until - now).total_seconds() + 1
                                self.append_log(project_id, f"â³ è´¦å· {next_ready.account_name} å†·å´ä¸­ï¼Œç­‰å¾… {wait_sec:.1f} ç§’...")
                                await asyncio.sleep(wait_sec)
                                account = next_ready

                        if not account:
                            if retry_num == 0:
                                self.append_log(project_id, f"âŒ å¹³å° {display_platform} æ²¡æœ‰å¯ç”¨è´¦å·ï¼Œè·³è¿‡")
                            else:
                                self.append_log(project_id, f"âŒ å¹³å° {display_platform} æ²¡æœ‰æ›´å¤šå¯ç”¨è´¦å·")
                            break
                        
                        tried_accounts.append(account.id)
                        self.append_log(project_id, f"âœ… è·å–åˆ°è´¦å·: {account.account_name}")
                        cookies = account.cookies
                    except Exception as e:
                        self.append_log(project_id, f"âŒ è·å–è´¦å·å¤±è´¥: {e}")
                        break
                    
                    # æ£€æŸ¥çˆ¬è™«çŠ¶æ€
                    if crawler_manager.status == "running":
                        self.append_log(project_id, f"âš ï¸ çˆ¬è™«å¼•æ“å¿™ç¢Œä¸­ï¼Œè·³è¿‡å¹³å° {display_platform}")
                        break
                    
                    try:
                        # æ˜ å°„å¹³å°åç§°åˆ° MediaCrawler æ”¯æŒçš„æ ¼å¼
                        platform_mapping = {
                            "douyin": "dy",
                            "bilibili": "bili",
                            "weibo": "wb",
                            "xhs": "xhs",
                            "kuaishou": "ks",
                            "zhihu": "zhihu",
                            "tieba": "tieba"
                        }
                        mc_platform = platform_mapping.get(platform, platform)
                        
                        self.append_log(project_id, f"ğŸš€ å¯åŠ¨çˆ¬è™«ä»»åŠ¡: {display_platform} - {project.crawler_type}")
                        
                        # è®¡ç®—åŠ¨æ€æ—¶é—´èŒƒå›´ (Dynamically calculate time range)
                        start_time_str = ""
                        start_time_str = ""
                        end_time_str = ""
                        if getattr(project, 'crawl_date_range', 0) > 0:
                            range_days = project.crawl_date_range
                            now = datetime.now()
                            start_date = now - timedelta(days=range_days)
                            start_time_str = start_date.strftime("%Y-%m-%d %H:%M:%S")
                            end_time_str = now.strftime("%Y-%m-%d %H:%M:%S")
                            self.append_log(project_id, f"ğŸ“… çˆ¬å–æ—¶é—´çª—å£: {start_time_str} è‡³ {end_time_str} (æœ€è¿‘ {range_days} å¤©)")
                        
                        config = CrawlerStartRequest(
                            platform=mc_platform,
                            login_type="cookie",
                            crawler_type=project.crawler_type or "search",
                            save_option="sqlite",
                            keywords=keywords_str,
                            cookies=cookies,
                            headless=False,
                            crawl_limit_count=project.crawl_limit or 20,
                            start_time=start_time_str,
                            end_time=end_time_str,
                            enable_comments=project.enable_comments if project.enable_comments is not None else True,
                            project_id=project.id,  # å…³è”é¡¹ç›® ID
                            # Pass interaction filters from project settings
                            min_likes=getattr(project, 'min_likes', 0) or 0,
                            min_comments=getattr(project, 'min_comments', 0) or 0,
                            min_shares=getattr(project, 'min_shares', 0) or 0,
                            min_favorites=getattr(project, 'min_favorites', 0) or 0,
                            max_likes=getattr(project, 'max_likes', 0) or 0,
                            max_comments=getattr(project, 'max_comments', 0) or 0,
                            max_shares=getattr(project, 'max_shares', 0) or 0,
                            max_favorites=getattr(project, 'max_favorites', 0) or 0,
                            deduplicate_authors=getattr(project, 'deduplicate_authors', False) or False,
                            concurrency_num=getattr(project, 'max_concurrency', 3) or 3,
                            account_id=str(account.id),
                        )

                        
                        # Log all config values before execution
                        self.append_log(project_id, f"ğŸ“‹ çˆ¬è™«é…ç½®å‚æ•°:")
                        self.append_log(project_id, f"   - å¹³å°: {mc_platform}, ç±»å‹: {config.crawler_type}")
                        self.append_log(project_id, f"   - æŠ“å–æ•°é‡: {config.crawl_limit_count}")
                        self.append_log(project_id, f"   - å¼€å§‹æ—¶é—´: {config.start_time or 'ä¸é™'}")
                        self.append_log(project_id, f"   - ç‚¹èµèŒƒå›´: {config.min_likes} ~ {config.max_likes if config.max_likes > 0 else 'ä¸é™'}")
                        self.append_log(project_id, f"   - è¯„è®ºèŒƒå›´: {config.min_comments} ~ {config.max_comments if config.max_comments > 0 else 'ä¸é™'}")
                        self.append_log(project_id, f"   - åˆ†äº«èŒƒå›´: {config.min_shares} ~ {config.max_shares if config.max_shares > 0 else 'ä¸é™'}")
                        self.append_log(project_id, f"   - æ”¶è—èŒƒå›´: {config.min_favorites} ~ {config.max_favorites if config.max_favorites > 0 else 'ä¸é™'}")
                        self.append_log(project_id, f"   - åšä¸»å»é‡: {'æ˜¯' if config.deduplicate_authors else 'å¦'}")
                        
                        success = await crawler_manager.start(config)
                        if success:
                            self.append_log(project_id, "çˆ¬è™«å·²æäº¤ï¼Œç­‰å¾…æ‰§è¡Œ...")
                            
                            # åŒæ­¥çˆ¬è™«æ—¥å¿—çš„æ¸¸æ ‡
                            last_log_count = 0
                            
                            # ç­‰å¾…å®Œæˆï¼Œå¹¶åŒæ­¥æ—¥å¿—
                            while crawler_manager.status == "running":
                                # è·å–æ–°äº§ç”Ÿçš„çˆ¬è™«æ—¥å¿—
                                current_logs = crawler_manager.logs
                                if len(current_logs) > last_log_count:
                                    new_logs = current_logs[last_log_count:]
                                    for log_entry in new_logs:
                                        # è¿‡æ»¤ä¸€äº›æ— ç”¨æ—¥å¿—
                                        if "Starting crawler" in log_entry.message: continue
                                        
                                        # æ ¼å¼åŒ–å¹¶æ·»åŠ åˆ°é¡¹ç›®æ—¥å¿—
                                        self.append_log(project_id, f"ğŸ•·ï¸ {log_entry.message}")
                                    
                                    last_log_count = len(current_logs)
                                
                                await asyncio.sleep(1)
                                
                            # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„æ—¥å¿—ï¼ˆä»»åŠ¡åˆšç»“æŸæ—¶ï¼‰
                            current_logs = crawler_manager.logs
                            if len(current_logs) > last_log_count:
                                new_logs = current_logs[last_log_count:]
                                for log_entry in new_logs:
                                    self.append_log(project_id, f"ğŸ•·ï¸ {log_entry.message}")
                            
                            # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
                            final_status = crawler_manager.status
                            if final_status == "completed":
                                 # è·å–æœ¬æ¬¡ä»»åŠ¡æŠ“å–åˆ°çš„å†…å®¹æ•°é‡
                                 platform_new_items = 0
                                 try:
                                     from database.growhub_models import GrowHubContent
                                     from sqlalchemy import func
                                     async with get_session() as session:
                                         # ç»Ÿè®¡è¯¥é¡¹ç›®è¯¥å¹³å°è‡ªä»»åŠ¡å¯åŠ¨ä»¥æ¥çš„æ–°å†…å®¹ (Count new contents for this project & platform since task start)
                                         count_result = await session.execute(
                                             select(func.count(GrowHubContent.id))
                                             .where(GrowHubContent.project_id == project_id)
                                             .where(GrowHubContent.platform == platform)
                                             .where(GrowHubContent.crawl_time >= start_time_utc)
                                         )
                                         platform_new_items = count_result.scalar() or 0
                                         total_crawled_items += platform_new_items
                                 except Exception as e:
                                     self.append_log(project_id, f"âš ï¸ ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")

                                 self.append_log(project_id, f"âœ… å¹³å° {display_platform} çˆ¬å–ä»»åŠ¡æˆåŠŸå®Œæˆï¼ŒæŠ“å– {platform_new_items} æ¡æ–°å†…å®¹")
                                 success_this_platform = True
                                 
                                 # æ›´æ–°è´¦å·æˆåŠŸæ¬¡æ•° (Sticky Sessions)
                                 await pool.record_account_usage(account.id, success=True, project_id=project_id)
                                 break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                            else:
                                # çˆ¬è™«å¤±è´¥
                                self.append_log(project_id, f"âš ï¸ çˆ¬è™«çŠ¶æ€å¼‚å¸¸: {final_status}ï¼Œå°è¯•åˆ‡æ¢è´¦å·...")
                                
                                # æ‰«ææ—¥å¿—æŸ¥æ‰¾ç‰¹å®šé”™è¯¯ (Auto-invalidate account on permission error)
                                has_permission_error = False
                                self.append_log(project_id, f"ğŸ” æ­£åœ¨æ£€æŸ¥ {len(crawler_manager.logs)} æ¡æ—¥å¿—ä»¥æŸ¥æ‰¾æƒé™é”™è¯¯...")
                                for entry in crawler_manager.logs:
                                    if "-104" in entry.message or "æ²¡æœ‰æƒé™" in entry.message:
                                        self.append_log(project_id, f"ğŸ” å‘ç°é”™è¯¯æ—¥å¿—: {entry.message[:50]}...")
                                        has_permission_error = True
                                        break
                                
                                if has_permission_error:
                                    self.append_log(project_id, f"ğŸš« æ£€æµ‹åˆ°è´¦å· {account.account_name} æƒé™å—é™ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ")
                                    await pool.update_account(account.id, {"status": AccountStatus.BANNED})
                                else:
                                    self.append_log(project_id, "ğŸ” æœªå‘ç°æƒé™ç›¸å…³é”™è¯¯")
                                await pool.record_account_usage(account.id, success=False, project_id=project_id)
                                # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                                
                    except Exception as e:
                        error_msg = str(e)
                        self.append_log(project_id, f"âŒ å¹³å° {display_platform} çˆ¬è™«æ‰§è¡Œå¼‚å¸¸: {error_msg}")
                        
                        # æ ‡è®°è´¦å·å¤±è´¥
                        try:
                            await pool.record_account_usage(account.id, success=False, project_id=project_id)
                        except:
                            pass
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¯è´¦å·ç›¸å…³çš„é”™è¯¯ï¼Œå†³å®šæ˜¯å¦é‡è¯•
                        account_errors = ["æ²¡æœ‰æƒé™", "Cookie", "403", "401", "406", "ç™»å½•"]
                        is_account_error = any(err in error_msg for err in account_errors)
                        
                        if is_account_error and retry_num < MAX_ACCOUNT_RETRIES - 1:
                            self.append_log(project_id, "ğŸ”„ æ£€æµ‹åˆ°è´¦å·é—®é¢˜ï¼Œå°è¯•åˆ‡æ¢è´¦å·...")
                            continue
                        else:
                            break
                
                if not success_this_platform:
                    self.append_log(project_id, f"âŒ å¹³å° {display_platform} æ‰€æœ‰è´¦å·å‡å¤±è´¥")

            
            # æ›´æ–°ç»Ÿè®¡ (Final Statistics Update)
            try:
                async with get_session() as session:
                    # æˆ‘ä»¬éœ€è¦é‡æ–°è·å– project å¯¹è±¡ï¼Œå› ä¸ºå®ƒå¯èƒ½å·²ç»è¿‡æœŸ
                    refresh_proj = await session.get(GrowHubProject, project_id)
                    if refresh_proj:
                        refresh_proj.total_crawled = (refresh_proj.total_crawled or 0) + total_crawled_items
                        refresh_proj.today_crawled = (refresh_proj.today_crawled or 0) + total_crawled_items
                        await session.commit()
            except Exception as e:
                print(f"Update stats error: {e}")

            self.append_log(project_id, "========================================")
            self.append_log(project_id, f"ğŸ“Š ä»»åŠ¡æ±‡æ€»æŠ¥å‘Š:")
            self.append_log(project_id, f"   - é¡¹ç›®åç§°: {project.name}")
            self.append_log(project_id, f"   - æ€»è®¡æŠ“å–: {total_crawled_items} æ¡æ–°å†…å®¹")
            self.append_log(project_id, f"   - è¿è¡Œè€—æ—¶: {(datetime.now() - start_time_local).total_seconds():.1f} ç§’")
            
            # --- Alert Processing ---
            if total_crawled_items > 0:
                try:
                    from api.services.alert import get_alert_service
                    from database.growhub_models import GrowHubContent
                    from sqlalchemy import select, and_
                    
                    alert_service = get_alert_service()
                    
                    if project.keywords:
                        # Use a new session or the context session if available?
                        # _execute_project is called within background task, session management is tricky.
                        # We use get_session() context manager.
                        from database.db_session import get_session
                        
                        async with get_session() as session:
                            result = await session.execute(
                                select(GrowHubContent).where(
                                    and_(
                                        GrowHubContent.created_at >= start_time_utc,
                                        GrowHubContent.source_keyword.in_(project.keywords)
                                    )
                                )
                            )
                            new_contents = result.scalars().all()
                            
                            if new_contents:
                                self.append_log(project_id, f"ğŸ”” å‘ç° {len(new_contents)} æ¡æ–°å†…å®¹ï¼Œæ­£åœ¨åˆ†æèˆ†æƒ…...")
                                alerts_count = await alert_service.process_project_alerts(project, new_contents)
                                
                                # Fetch project in this session to update counts
                                refresh_proj = await session.get(GrowHubProject, project_id)
                                if refresh_proj:
                                    refresh_proj.total_alerts = (refresh_proj.total_alerts or 0) + alerts_count
                                    refresh_proj.today_alerts = (refresh_proj.today_alerts or 0) + alerts_count
                                    await session.commit()
                                
                                self.append_log(project_id, f"ğŸ“© è§¦å‘ {alerts_count} æ¡é¢„è­¦é€šçŸ¥")
                            else:
                                self.append_log(project_id, "æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–°å†…å®¹ï¼Œè·³è¿‡é¢„è­¦")
                except Exception as e:
                    self.append_log(project_id, f"âŒ é¢„è­¦å¤„ç†å¤±è´¥: {e}")
            
            self.append_log(project_id, "ğŸ æœ¬æ¬¡è‡ªåŠ¨åŒ–ç›‘æ§ä»»åŠ¡å…¨éƒ¨æ‰§è¡Œç»“æŸ")
            self.append_log(project_id, "========================================")
            
            # è®¡ç®—ä¸‹æ¬¡è¿è¡Œæ—¶é—´
            if project.is_active and project.schedule_type == "interval":
                try:
                    interval = int(project.schedule_value)
                    project.next_run_at = datetime.now() + timedelta(seconds=interval)
                except:
                    pass
    
    async def _register_scheduler_task(self, project):
        """æ³¨å†Œè°ƒåº¦ä»»åŠ¡"""
        from api.services.scheduler import get_scheduler, ScheduledTask, TaskType
        
        scheduler = get_scheduler()
        
        task = ScheduledTask(
            id="",
            name=f"[é¡¹ç›®] {project.name}",
            task_type=TaskType.CRAWLER,
            description=f"ç›‘æ§é¡¹ç›®è‡ªåŠ¨ä»»åŠ¡: {project.name}",
            params={
                "project_id": project.id,
                "platforms": project.platforms,
                "keywords": project.keywords,
                "crawler_type": project.crawler_type,
                "limit_count": project.crawl_limit,
            }
        )
        
        if project.schedule_type == "interval":
            try:
                task.interval_seconds = int(project.schedule_value)
            except:
                task.interval_seconds = 3600
        elif project.schedule_type == "cron":
            task.cron_expression = project.schedule_value
        
        created_task = await scheduler.add_task(task)
        project.scheduler_task_id = created_task.id
        project.next_run_at = created_task.next_run
        
        print(f"[Project] å·²æ³¨å†Œè°ƒåº¦ä»»åŠ¡: {project.name} (ID: {created_task.id})")
    
    async def _unregister_scheduler_task(self, project):
        """å–æ¶ˆè°ƒåº¦ä»»åŠ¡"""
        if not project.scheduler_task_id:
            return
        
        from api.services.scheduler import get_scheduler
        
        scheduler = get_scheduler()
        await scheduler.delete_task(project.scheduler_task_id)
        project.scheduler_task_id = None
        project.next_run_at = None
        
        print(f"[Project] å·²å–æ¶ˆè°ƒåº¦ä»»åŠ¡: {project.name}")
    
    async def get_project_contents(self, project_id: int, page: int = 1, page_size: int = 20, 
                                 filters: Dict[str, Any] = None) -> Dict[str, Any]:
        """è·å–é¡¹ç›®å…³è”çš„å†…å®¹åˆ—è¡¨"""
        filters = filters or {}
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject, GrowHubContent
        from sqlalchemy import select, desc, func, and_, or_
        
        async with get_session() as session:
            # 1. è·å–é¡¹ç›®
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            if not project:
                return {"items": [], "total": 0, "error": "Project not found"}
            
            # 2. æ„å»ºæŸ¥è¯¢ - ä¼˜å…ˆä½¿ç”¨ project_id è¿‡æ»¤ï¼Œå¦åˆ™å›é€€åˆ°å…³é”®è¯åŒ¹é…
            # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ project_id å…³è”çš„å†…å®¹
            project_id_check = await session.execute(
                select(func.count(GrowHubContent.id)).where(GrowHubContent.project_id == project_id)
            )
            has_project_id_content = (project_id_check.scalar() or 0) > 0
            
            if has_project_id_content:
                # ä½¿ç”¨ project_id ç²¾ç¡®è¿‡æ»¤
                query = select(GrowHubContent).where(GrowHubContent.project_id == project_id)
                count_query = select(func.count(GrowHubContent.id)).where(GrowHubContent.project_id == project_id)
            else:
                # å›é€€åˆ°å…³é”®è¯åŒ¹é…ï¼ˆå‘åå…¼å®¹ï¼‰
                if not project.keywords:
                    return {"items": [], "total": 0, "page": page, "page_size": page_size}
                
                keywords = project.keywords
                conditions = [GrowHubContent.source_keyword.like(f"%{k}%") for k in keywords]
                query = select(GrowHubContent).where(or_(*conditions))
                count_query = select(func.count(GrowHubContent.id)).where(or_(*conditions))
            
            # 3. åº”ç”¨è¿‡æ»¤
            if filters:
                if filters.get("platform"):
                    query = query.where(GrowHubContent.platform == filters["platform"])
                    count_query = count_query.where(GrowHubContent.platform == filters["platform"])
                if filters.get("sentiment"):
                    query = query.where(GrowHubContent.sentiment == filters["sentiment"])
                    count_query = count_query.where(GrowHubContent.sentiment == filters["sentiment"])
            
            # 3.5 Apply Deduplication (Author)
            should_dedup = filters.get("deduplicate_authors")
            if should_dedup is None:
                should_dedup = project.deduplicate_authors
                
            if should_dedup:
                # Use Window Function to keep latest post per author
                subq = query.subquery()
                rn = func.row_number().over(
                    partition_by=subq.c.author_id,
                    order_by=desc(subq.c.publish_time)
                ).label("rn")
                
                cte = select(subq.c.id, rn).cte()
                
                # Rebuild Query and Count Query
                query = select(GrowHubContent).join(cte, GrowHubContent.id == cte.c.id).where(cte.c.rn == 1)
                count_query = select(func.count()).select_from(cte).where(cte.c.rn == 1)
            
            # 4. åˆ†é¡µå’Œæ’åº
            query = query.order_by(desc(GrowHubContent.publish_time))
            query = query.offset((page - 1) * page_size).limit(page_size)
            
            # 5. æ‰§è¡ŒæŸ¥è¯¢
            content_result = await session.execute(query)
            contents = content_result.scalars().all()
            
            total_result = await session.execute(count_query)
            total = total_result.scalar() or 0
            
            return {
                "items": self._contents_to_list(contents),
                "total": total,
                "page": page,
                "page_size": page_size
            }
            
    async def get_project_stats_chart(self, project_id: int, days: int = 7) -> Dict[str, Any]:
        """è·å–é¡¹ç›®å›¾è¡¨ç»Ÿè®¡æ•°æ®"""
        from database.db_session import get_session
        from database.growhub_models import GrowHubProject, GrowHubContent
        from sqlalchemy import select, func, and_
        
        async with get_session() as session:
            # 1. è·å–é¡¹ç›®
            result = await session.execute(
                select(GrowHubProject).where(GrowHubProject.id == project_id)
            )
            project = result.scalar()
            if not project or not project.keywords:
                return {"dates": [], "sentiment_trend": [], "platform_dist": []}
            
            keywords = project.keywords
            start_date = datetime.now() - timedelta(days=days)
            
            # 2. æƒ…æ„Ÿè¶‹åŠ¿ (æŒ‰æ—¥æœŸåˆ†ç»„)
            # SQLite çš„æ—¥æœŸå¤„ç†æ¯”è¾ƒç‰¹æ®Šï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼ŒåªæŸ¥æ•°æ®ç„¶ååœ¨å†…å­˜èšåˆ
            # ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨æ•°æ®åº“ç‰¹å®šçš„æ—¥æœŸå‡½æ•°
            date_query = select(
                GrowHubContent.publish_time, 
                GrowHubContent.sentiment
            ).where(
                and_(
                    GrowHubContent.source_keyword.in_(keywords),
                    GrowHubContent.publish_time >= start_date
                )
            )
            
            date_result = await session.execute(date_query)
            rows = date_result.all()
            
            # å†…å­˜èšåˆ
            daily_stats = {}
            for row in rows:
                if not row.publish_time:
                    continue
                date_str = row.publish_time.strftime("%Y-%m-%d")
                if date_str not in daily_stats:
                    daily_stats[date_str] = {"positive": 0, "neutral": 0, "negative": 0}
                
                sentiment = row.sentiment or "neutral"
                if sentiment in daily_stats[date_str]:
                    daily_stats[date_str][sentiment] += 1
            
            # è¡¥å…¨æ—¥æœŸ
            dates = []
            sentiment_trend = {"positive": [], "neutral": [], "negative": []}
            
            for i in range(days):
                d = (start_date + timedelta(days=i+1)).strftime("%Y-%m-%d")
                dates.append(d)
                stats = daily_stats.get(d, {"positive": 0, "neutral": 0, "negative": 0})
                sentiment_trend["positive"].append(stats["positive"])
                sentiment_trend["neutral"].append(stats["neutral"])
                sentiment_trend["negative"].append(stats["negative"])
                
            # 3. å¹³å°åˆ†å¸ƒ
            platform_query = select(
                GrowHubContent.platform,
                func.count(GrowHubContent.id)
            ).where(
                GrowHubContent.source_keyword.in_(keywords)
            ).group_by(GrowHubContent.platform)
            
            plat_result = await session.execute(platform_query)
            platform_dist = [{"name": row[0], "value": row[1]} for row in plat_result.all()]
            
            return {
                "dates": dates,
                "sentiment_trend": sentiment_trend,
                "platform_dist": platform_dist
            }

    def _contents_to_list(self, contents) -> List[Dict[str, Any]]:
        return [
            {
                "id": c.id,
                "platform": c.platform,
                "title": c.title,
                "description": (c.description[:200] + "...") if c.description and len(c.description) > 200 else c.description,
                "url": c.content_url,
                "author": c.author_name,
                "author_id": c.author_id,
                "author_avatar": c.author_avatar,
                "author_fans": c.author_fans_count,
                "author_likes": c.author_likes_count,
                "cover_url": c.cover_url,
                "publish_time": (c.publish_time.replace(tzinfo=timezone.utc).isoformat() if c.publish_time else None),
                "crawl_time": (c.crawl_time.replace(tzinfo=timezone.utc).isoformat() if c.crawl_time else None),  # Fix: add missing crawl_time
                "sentiment": c.sentiment,
                "view_count": c.view_count,
                "like_count": c.like_count,
                "comment_count": c.comment_count,
                "share_count": c.share_count,
                "collect_count": c.collect_count,
                "is_alert": c.is_alert,
                "source_keyword": c.source_keyword,
                # æ–°å¢å­—æ®µï¼šæ”¯æŒè§†é¢‘æ’­æ”¾å’Œåª’ä½“ç±»å‹æ˜¾ç¤º
                "content_type": c.content_type,
                "video_url": c.video_url,
                "media_urls": c.media_urls,
            }
            for c in contents
        ]

    def _to_dict(self, project) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": project.id,
            "name": project.name,
            "description": project.description,
            "keywords": project.keywords or [],
            "sentiment_keywords": project.sentiment_keywords or [],
            "platforms": project.platforms or [],
            "crawler_type": project.crawler_type,
            "crawl_limit": project.crawl_limit,
            "crawl_date_range": project.crawl_date_range,
            "enable_comments": project.enable_comments,
            "deduplicate_authors": project.deduplicate_authors,
            "schedule_type": project.schedule_type,
            "schedule_value": project.schedule_value,
            "is_active": project.is_active,
            "alert_on_negative": project.alert_on_negative,
            "alert_on_hotspot": project.alert_on_hotspot,
            "alert_channels": project.alert_channels or [],
            
            # Advanced Filters
            "min_likes": project.min_likes or 0,
            "max_likes": project.max_likes or 0,
            "min_comments": project.min_comments or 0,
            "max_comments": project.max_comments or 0,
            "min_shares": project.min_shares or 0,
            "max_shares": project.max_shares or 0,
            "min_favorites": project.min_favorites or 0,
            "max_favorites": project.max_favorites or 0,
            "last_run_at": project.last_run_at.isoformat() if project.last_run_at else None,
            "next_run_at": project.next_run_at.isoformat() if project.next_run_at else None,
            "run_count": project.run_count or 0,
            "total_crawled": project.total_crawled or 0,
            "total_alerts": project.total_alerts or 0,
            "today_crawled": project.today_crawled or 0,
            "today_alerts": project.today_alerts or 0,
            "created_at": project.created_at.isoformat() if project.created_at else None,
            "updated_at": project.updated_at.isoformat() if project.updated_at else None,
        }


# å…¨å±€å®ä¾‹
project_service = ProjectService()


def get_project_service() -> ProjectService:
    """è·å–é¡¹ç›®æœåŠ¡å®ä¾‹"""
    return project_service
