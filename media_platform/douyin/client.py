# -*- coding: utf-8 -*-
# Copyright (c) 2025 relakkes@gmail.com
#
# This file is part of MediaCrawler project.
# Repository: https://github.com/NanmiCoder/MediaCrawler/blob/main/media_platform/douyin/client.py
# GitHub: https://github.com/NanmiCoder
# Licensed under NON-COMMERCIAL LEARNING LICENSE 1.1
#

# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚

import asyncio
import copy
import json
import urllib.parse
from datetime import datetime
from typing import TYPE_CHECKING, Any, Callable, Dict, Union, Optional

import httpx
from playwright.async_api import BrowserContext

from base.base_crawler import AbstractApiClient
from proxy.proxy_mixin import ProxyRefreshMixin
from tools import utils
from var import request_keyword_var

if TYPE_CHECKING:
    from proxy.proxy_ip_pool import ProxyIpPool

from .exception import *
from .field import *
from .help import *
from tenacity import retry, stop_after_attempt, wait_exponential, before_sleep_log
import time

class AsyncTokenBucket:
    """å¼‚æ­¥ä»¤ç‰Œæ¡¶é™æµå™¨ (Async Token Bucket Rate Limiter)"""
    def __init__(self, rate: float, capacity: float):
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.monotonic()
        self._lock = asyncio.Lock()

    async def consume(self, amount: float = 1.0):
        async with self._lock:
            while self.tokens < amount:
                now = time.monotonic()
                delta = (now - self.last_update) * self.rate
                self.tokens = min(self.capacity, self.tokens + delta)
                self.last_update = now
                if self.tokens < amount:
                    sleep_time = (amount - self.tokens) / self.rate
                    await asyncio.sleep(sleep_time)
            self.tokens -= amount

class DouYinClient(AbstractApiClient, ProxyRefreshMixin):

    def __init__(
        self,
        timeout=60,  # è‹¥å¼€å¯çˆ¬å–åª’ä½“é€‰é¡¹ï¼ŒæŠ–éŸ³çš„çŸ­è§†é¢‘éœ€è¦æ›´ä¹…çš„è¶…æ—¶æ—¶é—´
        proxy=None,
        *,
        headers: Dict,
        playwright_page: Optional[Page],
        cookie_dict: Dict,
        proxy_ip_pool: Optional["ProxyIpPool"] = None,
    ):
        self.proxy = proxy
        self.timeout = timeout
        self.headers = headers
        self._host = "https://www.douyin.com"
        self.playwright_page = playwright_page
        self.cookie_dict = cookie_dict
        # åˆå§‹åŒ–é™æµå™¨ (TPS é™åˆ¶)
        import config
        global_tps = getattr(config, "GLOBAL_TPS_LIMIT", 1.5)
        self.rate_limiter = AsyncTokenBucket(rate=global_tps, capacity=global_tps * 2)
        
        # è®°å½•ä¸Šä¸€æ¬¡è¯·æ±‚çš„ Referer ç”¨äºé“¾è·¯æ¨¡æ‹Ÿ
        self.last_referer = "https://www.douyin.com/"
        
        # åˆå§‹åŒ–ä»£ç†æ± ï¼ˆæ¥è‡ª ProxyRefreshMixinï¼‰
        # Pro Feature: Pass ACCOUNT_ID for IP-Account affinity
        self.init_proxy_pool(proxy_ip_pool, account_id=config.ACCOUNT_ID)


    async def __process_req_params(
        self,
        uri: str,
        params: Optional[Dict] = None,
        headers: Optional[Dict] = None,
        request_method="GET",
    ):

        if not params:
            return
        headers = headers or self.headers
        local_storage: Dict = await self.playwright_page.evaluate("() => window.localStorage")  # type: ignore
        import config
        common_params = {
            "device_platform": "webapp",
            "aid": "6383",
            "channel": "channel_pc_web",
            "publish_video_strategy_type": 2,
            "update_version_code": 170400,
            "pc_client_type": 1,
            "version_code": "190600",
            "version_name": "19.6.0",
            "cookie_enabled": "true",
            "screen_width": 2560,
            "screen_height": 1440,
            "browser_language": "zh-CN",
            "browser_platform": "MacIntel",
            "browser_name": "Chrome",
            "browser_version": "135.0.0.0",
            "browser_online": "true",
            "engine_name": "Blink",
            "engine_version": "135.0.0.0",
            "os_name": "Mac OS",
            "os_version": "10.15.7",
            "cpu_core_num": 8,
            "device_memory": 8,
            "platform": "PC",
            "downlink": 4.45,
            "effective_type": "4g",
            "round_trip_time": 100,
            "webid": get_web_id(),
            "msToken": local_storage.get("xmst"),
        }
        params.update(common_params)
        query_string = urllib.parse.urlencode(params)

        # 20240927 a-bogusæ›´æ–°ï¼ˆJSç‰ˆæœ¬ï¼‰
        post_data = {}
        if request_method == "POST":
            post_data = params

        # 202410: Enable signatures for ALL endpoints including search to avoid verify_check
        a_bogus = await get_a_bogus(
            url=uri,
            params=query_string,
            post_data=post_data,
            user_agent=headers.get("User-Agent"),
            page=self.playwright_page
        )
        if a_bogus:
            params["a_bogus"] = a_bogus
        else:
            # Fallback to playwright evaluation if JS signature fails
            utils.logger.debug(f"[DouYinClient] JS signature failed for {uri}, trying playwright...")
            a_bogus = await get_a_bogus_from_playright(query_string, post_data, headers.get("User-Agent"), self.playwright_page)
            if a_bogus:
                params["a_bogus"] = a_bogus

    async def update_account_status(self, status: str):
        """Update account status in DB so API process can see it"""
        import config
        account_id = getattr(config, "ACCOUNT_ID", None)
        if not account_id:
            return
            
        try:
            from database.db_session import get_session
            from database.growhub_models import GrowHubAccount
            from sqlalchemy import update
            
            async with get_session() as session:
                await session.execute(
                    update(GrowHubAccount)
                    .where(GrowHubAccount.id == account_id)
                    .values(
                        status=status,
                        updated_at=datetime.now()
                    )
                )
                await session.commit()
                utils.logger.warning(f"ğŸš¨ [DouYinClient] Account {account_id} status updated to: {status}")
        except Exception as e:
            utils.logger.error(f"[DouYinClient] Failed to update account status in DB: {e}")

    @retry(
        stop=stop_after_attempt(3), 
        wait=wait_exponential(multiplier=1, min=2, max=10),
        before_sleep=before_sleep_log(utils.logger, "DEBUG")
    )
    async def request(self, method, url, **kwargs):
        # 1. è§¦å‘é¢‘ç‡é™åˆ¶ (Token Bucket)
        await self.rate_limiter.consume()
        
        # 2. åˆ·æ–°è¿‡æœŸä»£ç†
        await self._refresh_proxy_if_expired()

        # 3. åŠ¨æ€ Referer æ³¨å…¥ (é“¾è·¯æ¨¡æ‹Ÿ)
        headers = kwargs.get("headers", {})
        if "Referer" not in headers:
            headers["Referer"] = self.last_referer
            kwargs["headers"] = headers

        async with httpx.AsyncClient(proxy=self.proxy) as client:
            response = await client.request(method, url, timeout=self.timeout, **kwargs)
        
        # è®°å½•æœ€æ–°çš„ Referer (å¦‚æœæ˜¯ GET HTML é¡µé¢æˆ–ä¸»æ¥å£)
        if method == "GET" and "/web/" not in url:
            self.last_referer = url

        try:
            # æŠ–éŸ³å¸¸è§çš„æ‹¦æˆªå…³é”®è¯
            blocked_keywords = ["blocked", "verify_check", "verify_check_s", "forbidden"]
            res_text = response.text.lower()
            
            if response.text == "" or any(k in res_text for k in blocked_keywords):
                utils.logger.warning(f"ğŸš¨ [DouYinClient] æ£€æµ‹åˆ°é£æ§æ‹¦æˆªæˆ–ç©ºå›å¤! URL: {url}, Response: {response.text[:100]}")
                # è®°å½•è´¦å·è¿›å…¥å†·å´çŠ¶æ€
                await self.update_account_status("cooldown")
                raise Exception(f"account blocked or anti-bot triggered: {response.text[:50]}")
                
            return response.json()
        except Exception as e:
            if "anti-bot" in str(e) or "blocked" in str(e):
                raise e
            raise DataFetchError(f"{e}, {response.text[:200]}")


    async def get(self, uri: str, params: Optional[Dict] = None, headers: Optional[Dict] = None):
        """
        GETè¯·æ±‚
        """
        # å¦‚æœæ˜¯ä¸» API è¯·æ±‚ï¼Œé€šè¿‡ Referer é“¾è·¯æ¨¡æ‹ŸçœŸå®çš„è·³è½¬æ¥æº
        # æ¯”å¦‚æœç´¢å®Œåï¼ŒReferer åº”è¯¥æ˜¯æœç´¢ç»“æœé¡µ
        await self.__process_req_params(uri, params, headers)
        headers = headers or self.headers
        full_url = f"{self._host}{uri}"
        return await self.request(method="GET", url=full_url, params=params, headers=headers)

    async def post(self, uri: str, data: dict, headers: Optional[Dict] = None):
        await self.__process_req_params(uri, data, headers)
        headers = headers or self.headers
        return await self.request(method="POST", url=f"{self._host}{uri}", data=data, headers=headers)

    async def pong(self, browser_context: BrowserContext) -> bool:
        local_storage = await self.playwright_page.evaluate("() => window.localStorage")
        if local_storage.get("HasUserLogin", "") == "1":
            return True

        _, cookie_dict = utils.convert_cookies(await browser_context.cookies())
        return cookie_dict.get("LOGIN_STATUS") == "1"

    async def update_cookies(self, browser_context: BrowserContext):
        cookie_str, cookie_dict = utils.convert_cookies(await browser_context.cookies())
        self.headers["Cookie"] = cookie_str
        self.cookie_dict = cookie_dict

    async def search_info_by_keyword(
        self,
        keyword: str,
        offset: int = 0,
        search_channel: SearchChannelType = SearchChannelType.GENERAL,
        sort_type: SearchSortType = SearchSortType.GENERAL,
        publish_time: PublishTimeType = PublishTimeType.UNLIMITED,
        search_id: str = "",
    ):
        """
        DouYin Web Search API
        :param keyword:
        :param offset:
        :param search_channel:
        :param sort_type:
        :param publish_time: Â·
        :param search_id: Â·
        :return:
        """
        query_params = {
            'search_channel': search_channel.value,
            'enable_history': '1',
            'keyword': keyword,
            'search_source': 'tab_search',
            'query_correct_type': '1',
            'is_filter_search': '0',
            'from_group_id': '7378810571505847586',
            'offset': offset,
            'count': '20',
            'need_filter_settings': '1',
            'list_type': 'multi',
            'search_id': search_id,
        }
        if sort_type.value != SearchSortType.GENERAL.value or publish_time.value != PublishTimeType.UNLIMITED.value:
            query_params["filter_selected"] = json.dumps({"sort_type": str(sort_type.value), "publish_time": str(publish_time.value)})
            query_params["is_filter_search"] = 1
            query_params["search_source"] = "tab_search"
        referer_url = f"https://www.douyin.com/search/{keyword}?aid=f594bbd9-a0e2-4651-9319-ebe3cb6298c1&type=general"
        headers = copy.copy(self.headers)
        headers["Referer"] = urllib.parse.quote(referer_url, safe=':/')
        return await self.get("/aweme/v1/web/general/search/single/", query_params, headers=headers)

    async def get_homefeed(self, cursor: int = 0, count: int = 10, refresh_type: int = 1) -> Dict:
        """
        Get Douyin home feed (recommended videos)
        
        :param cursor: Pagination cursor (offset)
        :param count: Number of items per page
        :param refresh_type: 1 for initial load, 4 for load more
        :return: Home feed response with aweme_list
        """
        uri = "/aweme/v1/web/tab/feed/"
        params = {
            "count": count,
            "cursor": cursor,
            "refresh_type": refresh_type,
            "type_id": "",
            "max_cursor": cursor,
            "min_cursor": 0,
            "aweme_pc_rec_raw_data": "{}",
            "pull_type": 1 if cursor == 0 else 2,
            "is_from_gallery": "false",
        }
        headers = copy.copy(self.headers)
        headers["Referer"] = "https://www.douyin.com/"
        return await self.get(uri, params, headers)

    async def get_video_by_id(self, aweme_id: str) -> Any:
        """
        DouYin Video Detail API
        :param aweme_id:
        :return:
        """
        params = {"aweme_id": aweme_id}
        headers = copy.copy(self.headers)
        del headers["Origin"]
        res = await self.get("/aweme/v1/web/aweme/detail/", params, headers)
        return res.get("aweme_detail", {})

    async def get_aweme_comments(self, aweme_id: str, cursor: int = 0):
        """get note comments

        """
        uri = "/aweme/v1/web/comment/list/"
        params = {"aweme_id": aweme_id, "cursor": cursor, "count": 20, "item_type": 0}
        keywords = request_keyword_var.get()
        referer_url = "https://www.douyin.com/search/" + keywords + '?aid=3a3cec5a-9e27-4040-b6aa-ef548c2c1138&publish_time=0&sort_type=0&source=search_history&type=general'
        headers = copy.copy(self.headers)
        headers["Referer"] = urllib.parse.quote(referer_url, safe=':/')
        return await self.get(uri, params)

    async def get_sub_comments(self, aweme_id: str, comment_id: str, cursor: int = 0):
        """
            è·å–å­è¯„è®º
        """
        uri = "/aweme/v1/web/comment/list/reply/"
        params = {
            'comment_id': comment_id,
            "cursor": cursor,
            "count": 20,
            "item_type": 0,
            "item_id": aweme_id,
        }
        keywords = request_keyword_var.get()
        referer_url = "https://www.douyin.com/search/" + keywords + '?aid=3a3cec5a-9e27-4040-b6aa-ef548c2c1138&publish_time=0&sort_type=0&source=search_history&type=general'
        headers = copy.copy(self.headers)
        headers["Referer"] = urllib.parse.quote(referer_url, safe=':/')
        return await self.get(uri, params)

    async def get_aweme_all_comments(
        self,
        aweme_id: str,
        crawl_interval: float = 1.0,
        is_fetch_sub_comments=False,
        callback: Optional[Callable] = None,
        max_count: int = 10,
    ):
        """
        è·å–å¸–å­çš„æ‰€æœ‰è¯„è®ºï¼ŒåŒ…æ‹¬å­è¯„è®º
        :param aweme_id: å¸–å­ID
        :param crawl_interval: æŠ“å–é—´éš”
        :param is_fetch_sub_comments: æ˜¯å¦æŠ“å–å­è¯„è®º
        :param callback: å›è°ƒå‡½æ•°ï¼Œç”¨äºå¤„ç†æŠ“å–åˆ°çš„è¯„è®º
        :param max_count: ä¸€æ¬¡å¸–å­çˆ¬å–çš„æœ€å¤§è¯„è®ºæ•°é‡
        :return: è¯„è®ºåˆ—è¡¨
        """
        result = []
        comments_has_more = 1
        comments_cursor = 0
        while comments_has_more and len(result) < max_count:
            comments_res = await self.get_aweme_comments(aweme_id, comments_cursor)
            comments_has_more = comments_res.get("has_more", 0)
            comments_cursor = comments_res.get("cursor", 0)
            comments = comments_res.get("comments", [])
            if not comments:
                continue
            if len(result) + len(comments) > max_count:
                comments = comments[:max_count - len(result)]
            result.extend(comments)
            if callback:  # å¦‚æœæœ‰å›è°ƒå‡½æ•°ï¼Œå°±æ‰§è¡Œå›è°ƒå‡½æ•°
                await callback(aweme_id, comments)

            await asyncio.sleep(crawl_interval)
            if not is_fetch_sub_comments:
                continue
            # è·å–äºŒçº§è¯„è®º
            for comment in comments:
                reply_comment_total = comment.get("reply_comment_total")

                if reply_comment_total > 0:
                    comment_id = comment.get("cid")
                    sub_comments_has_more = 1
                    sub_comments_cursor = 0

                    while sub_comments_has_more:
                        sub_comments_res = await self.get_sub_comments(aweme_id, comment_id, sub_comments_cursor)
                        sub_comments_has_more = sub_comments_res.get("has_more", 0)
                        sub_comments_cursor = sub_comments_res.get("cursor", 0)
                        sub_comments = sub_comments_res.get("comments", [])

                        if not sub_comments:
                            continue
                        result.extend(sub_comments)
                        if callback:  # å¦‚æœæœ‰å›è°ƒå‡½æ•°ï¼Œå°±æ‰§è¡Œå›è°ƒå‡½æ•°
                            await callback(aweme_id, sub_comments)
                        await asyncio.sleep(crawl_interval)
        return result

    async def get_user_info(self, sec_user_id: str):
        uri = "/aweme/v1/web/user/profile/other/"
        params = {
            "sec_user_id": sec_user_id,
            "publish_video_strategy_type": 2,
            "personal_center_strategy": 1,
        }
        headers = copy.copy(self.headers)
        headers["Referer"] = f"https://www.douyin.com/user/{sec_user_id}"
        return await self.get(uri, params, headers)

    async def get_user_aweme_posts(self, sec_user_id: str, max_cursor: str = "") -> Dict:
        uri = "/aweme/v1/web/aweme/post/"
        params = {
            "sec_user_id": sec_user_id,
            "count": 18,
            "max_cursor": max_cursor,
            "locate_query": "false",
            "publish_video_strategy_type": 2,
            'verifyFp': 'verify_ma3hrt8n_q2q2HyYA_uLyO_4N6D_BLvX_E2LgoGmkA1BU',
            'fp': 'verify_ma3hrt8n_q2q2HyYA_uLyO_4N6D_BLvX_E2LgoGmkA1BU'
        }
        return await self.get(uri, params)

    async def get_all_user_aweme_posts(self, sec_user_id: str, callback: Optional[Callable] = None):
        posts_has_more = 1
        max_cursor = ""
        result = []
        while posts_has_more == 1:
            aweme_post_res = await self.get_user_aweme_posts(sec_user_id, max_cursor)
            posts_has_more = aweme_post_res.get("has_more", 0)
            max_cursor = aweme_post_res.get("max_cursor")
            aweme_list = aweme_post_res.get("aweme_list") if aweme_post_res.get("aweme_list") else []
            utils.logger.info(f"[DouYinClient.get_all_user_aweme_posts] get sec_user_id:{sec_user_id} video len : {len(aweme_list)}")
            if callback:
                await callback(aweme_list)
            result.extend(aweme_list)
        return result

    async def get_aweme_media(self, url: str) -> Union[bytes, None]:
        async with httpx.AsyncClient(proxy=self.proxy) as client:
            try:
                response = await client.request("GET", url, timeout=self.timeout, follow_redirects=True)
                response.raise_for_status()
                if not response.reason_phrase == "OK":
                    utils.logger.error(f"[DouYinClient.get_aweme_media] request {url} err, res:{response.text}")
                    return None
                else:
                    return response.content
            except httpx.HTTPError as exc:  # some wrong when call httpx.request method, such as connection error, client error, server error or response status code is not 2xx
                utils.logger.error(f"[DouYinClient.get_aweme_media] {exc.__class__.__name__} for {exc.request.url} - {exc}")  # ä¿ç•™åŸå§‹å¼‚å¸¸ç±»å‹åç§°ï¼Œä»¥ä¾¿å¼€å‘è€…è°ƒè¯•
                return None

    async def resolve_short_url(self, short_url: str) -> str:
        """
        è§£ææŠ–éŸ³çŸ­é“¾æ¥,è·å–é‡å®šå‘åçš„çœŸå®URL
        Args:
            short_url: çŸ­é“¾æ¥,å¦‚ https://v.douyin.com/iF12345ABC/
        Returns:
            é‡å®šå‘åçš„å®Œæ•´URL
        """
        async with httpx.AsyncClient(proxy=self.proxy, follow_redirects=False) as client:
            try:
                utils.logger.info(f"[DouYinClient.resolve_short_url] Resolving short URL: {short_url}")
                response = await client.get(short_url, timeout=10)

                # çŸ­é“¾æ¥é€šå¸¸è¿”å›302é‡å®šå‘
                if response.status_code in [301, 302, 303, 307, 308]:
                    redirect_url = response.headers.get("Location", "")
                    utils.logger.info(f"[DouYinClient.resolve_short_url] Resolved to: {redirect_url}")
                    return redirect_url
                else:
                    utils.logger.warning(f"[DouYinClient.resolve_short_url] Unexpected status code: {response.status_code}")
                    return ""
            except Exception as e:
                utils.logger.error(f"[DouYinClient.resolve_short_url] Failed to resolve short URL: {e}")
                return ""
