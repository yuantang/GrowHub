# -*- coding: utf-8 -*-

import asyncio
from datetime import datetime
from typing import List, TYPE_CHECKING

import config
from tools import utils
from media_platform.douyin.field import PublishTimeType, SearchSortType, SearchChannelType
from media_platform.douyin.exception import DataFetchError
from var import request_keyword_var, source_keyword_var, min_fans_var, max_fans_var, require_contact_var, sentiment_keywords_var
from media_platform.douyin.extractor import DouyinExtractor

if TYPE_CHECKING:
    from media_platform.douyin.client import DouYinClient
    from media_platform.douyin.processors.aweme_processor import AwemeProcessor
    from media_platform.douyin.processors.comment_processor import CommentProcessor
    from checkpoint.manager import CheckpointManager


class SearchHandler:
    def __init__(
        self,
        dy_client: "DouYinClient",
        checkpoint_manager: "CheckpointManager",
        aweme_processor: "AwemeProcessor",
        comment_processor: "CommentProcessor",
    ):
        self.dy_client = dy_client
        self.checkpoint_manager = checkpoint_manager
        self.aweme_processor = aweme_processor
        self.comment_processor = comment_processor
        self.extractor = DouyinExtractor()

    async def handle(self):
        """
        Execute search crawler
        """
        utils.logger.info("ğŸš€ [SearchHandler] å¼€å§‹æ‰§è¡ŒæŠ–éŸ³å…³é”®è¯æœç´¢ä»»åŠ¡")
        
        # 1. å‡†å¤‡å…³é”®è¯åˆ—è¡¨ (Prepare keywords with expansion)
        base_keywords = [k.strip() for k in config.KEYWORDS.split(",") if k.strip()]
        sentiment_keywords = sentiment_keywords_var.get() or []
        
        # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæä¾›äº†èˆ†æƒ…ç›‘æ§è¯ï¼Œåˆ™è¿›è¡ŒæŸ¥è¯¢æ‰©å±•
        # ç­–ç•¥ï¼šä¼˜å…ˆæœç´¢ "å…³é”®è¯ + èˆ†æƒ…è¯" çš„ç»„åˆï¼Œè¿™æ ·å¬å›ç‡æœ€é«˜ä¸”æœ€ç²¾å‡†
        search_keywords = []
        if sentiment_keywords:
            for kw in base_keywords:
                for skw in sentiment_keywords:
                    # ç»„åˆæœç´¢è¯ï¼Œä¾‹å¦‚ "Nowå†¥æƒ³ é€€æ¬¾"
                    search_keywords.append(f"{kw} {skw}")
            
            # æœ€åä¿ç•™åŸå§‹å…³é”®è¯ï¼Œä½œä¸ºä¸€ä¸ªå®½æ³›çš„è¡¥å……
            for kw in base_keywords:
                search_keywords.append(kw)
        else:
            search_keywords = base_keywords

        # å½’ä¸€åŒ–å»é‡
        search_keywords = list(dict.fromkeys(search_keywords))

        sentiment_display = ", ".join(sentiment_keywords) if sentiment_keywords else "æ— "

        utils.logger.info("ğŸ“‹ ä»»åŠ¡æ‰§è¡Œæ¡ä»¶ (å·²ä¼˜åŒ–èˆ†æƒ…æœç´¢):")
        utils.logger.info(f"   - åŸå§‹å…³é”®è¯: {config.KEYWORDS}")
        utils.logger.info(f"   - æœç´¢è¯é˜Ÿåˆ—: {search_keywords}")
        utils.logger.info(f"   - èˆ†æƒ…ç›‘æ§è¯: {sentiment_display}")
        utils.logger.info(f"   - çˆ¬å–æ€»é‡é™åˆ¶: {config.CRAWLER_MAX_NOTES_COUNT}")
        utils.logger.info(f"   - å‘å¸ƒæ—¶é—´èŒƒå›´: {config.START_TIME or 'ä¸é™'} è‡³ {config.END_TIME or 'ä¸é™'}")
        utils.logger.info(f"   - äº’åŠ¨è¦æ±‚: ç‚¹èµ>{config.MIN_LIKES_COUNT}, è¯„è®º>{config.MIN_COMMENTS_COUNT}")
        utils.logger.info(f"   - åšä¸»å»é‡: {'å¼€å¯' if config.DEDUPLICATE_AUTHORS else 'å…³é—­'}")
        
        # Get advanced filter vars
        min_fans = min_fans_var.get() or 0
        max_fans = max_fans_var.get() or 0
        require_contact = require_contact_var.get() or False

        # Config validation and defaults
        dy_limit_count = 20 # Douyin search count
        start_page = config.START_PAGE
        
        # Parse start/end time to timestamps for strict comparison
        start_timestamp = 0
        end_timestamp = 0
        
        def parse_timestamp(time_str: str) -> int:
            if not time_str: return 0
            try:
                if len(time_str) <= 10:
                    dt = datetime.strptime(time_str, "%Y-%m-%d")
                else:
                    dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
                return int(dt.timestamp())
            except Exception as e:
                utils.logger.error(f"âŒ è§£ææ—¶é—´ '{time_str}' å¤±è´¥: {e}")
                return 0

        start_timestamp = parse_timestamp(config.START_TIME)
        end_timestamp = parse_timestamp(config.END_TIME)
        
        utils.logger.info(f"â° è§£æç»“æœ: èµ·å§‹æ—¶é—´æˆ³={start_timestamp}, ç»“æŸæ—¶é—´æˆ³={end_timestamp}")

        # Task-level state
        total_processed_count = 0
        processed_authors = set()
        
        # --- å¾ªç¯æ‰§è¡Œæœç´¢è¯é˜Ÿåˆ— ---
        for keyword in search_keywords:
            if total_processed_count >= config.CRAWLER_MAX_NOTES_COUNT:
                 break
                 
            utils.logger.info(f"ğŸ” [SearchHandler] æ­£åœ¨æœç´¢: '{keyword}'")
            request_keyword_var.set(keyword)
            source_keyword_var.set(keyword)
            
            # æ˜¯å¦æ˜¯é’ˆå¯¹ç‰¹å®šèˆ†æƒ…è¯çš„æœç´¢
            is_expanded_query = any(skw in keyword for skw in sentiment_keywords) if sentiment_keywords else False
            
            checkpoint = await self.checkpoint_manager.find_matching_checkpoint(
                platform="douyin",
                crawler_type="search",
                keywords=keyword,
                project_id=config.PROJECT_ID if hasattr(config, "PROJECT_ID") else None
            )
            
            if not checkpoint:
                checkpoint = await self.checkpoint_manager.create_checkpoint(
                    platform="douyin",
                    crawler_type="search",
                    keywords=keyword,
                    project_id=config.PROJECT_ID if hasattr(config, "PROJECT_ID") else None
                )
            
            # Resume logic
            current_page = checkpoint.current_page
            page = max(current_page, start_page)
            dy_search_id = checkpoint.metadata.get("dy_search_id", "")
            has_more = True
            empty_retry_count = 0 
            
            while total_processed_count < config.CRAWLER_MAX_NOTES_COUNT and page <= start_page + 100:
                utils.logger.info(f"ğŸ“„ [SearchHandler] è¯·æ±‚ç¬¬ {page} é¡µ (åˆæ ¼è¿›åº¦: {total_processed_count}/{config.CRAWLER_MAX_NOTES_COUNT})")
                
                try:
                    # ã€ä¼˜åŒ–ã€‘ä¼˜å…ˆæ–°é²œåº¦ã€‚åŒæ—¶ä¹Ÿè®¾ç½® api_publish_time 
                    api_publish_time = PublishTimeType.UNLIMITED
                    if start_timestamp > 0:
                        now_ts = int(datetime.now().timestamp())
                        delta_days = (now_ts - start_timestamp) // 86400
                        if delta_days <= 1: api_publish_time = PublishTimeType.ONE_DAY
                        elif delta_days <= 7: api_publish_time = PublishTimeType.ONE_WEEK
                        elif delta_days <= 180: api_publish_time = PublishTimeType.SIX_MONTH
                    
                    # ã€æ ¸å¿ƒç­–ç•¥ã€‘å¦‚æœç¬¬ä¸€é¡µç»“æœå¤ªå°‘ï¼Œåç»­é¡µç åˆ‡æ¢åˆ° GENERAL é¢‘é“è·å–å…¨é‡
                    search_channel = SearchChannelType.VIDEO if page == 1 else SearchChannelType.GENERAL
                    
                    post_sort_type = SearchSortType(config.SORT_TYPE) if hasattr(config, "SORT_TYPE") else SearchSortType.GENERAL
                    if start_timestamp > 0 and page == 1:
                        post_sort_type = SearchSortType.LATEST

                    posts_res = await self.dy_client.search_info_by_keyword(
                        keyword=keyword,
                        offset=(page - 1) * dy_limit_count,
                        search_channel=search_channel,
                        sort_type=post_sort_type,
                        publish_time=api_publish_time,
                        search_id=dy_search_id,
                    )
                    
                    # ä¼˜å…ˆè·å– search_id è¿›è¡Œç¿»é¡µä¼šè¯ç»´æŒ
                    extra = posts_res.get("extra", {})
                    dy_search_id = extra.get("search_id") or extra.get("logid") or dy_search_id
                    has_more = posts_res.get("has_more") == 1 or posts_res.get("has_more") is True
                    checkpoint.metadata["dy_search_id"] = dy_search_id

                    data_list = posts_res.get("data", [])
                    total_raw = len(data_list)
                    
                    # ã€æ ¸å¿ƒè°ƒè¯•ã€‘é›†æˆ Pro ç‰ˆå®¡è®¡ï¼šæ‰“å°ç¬¬ä¸€é¡µå†…å®¹çš„åŸå§‹å¿«ç…§åŒ… (å¤šçº§è§£æ)
                    if data_list and page == start_page:
                        utils.logger.info("ğŸ“¦ [å®¡è®¡] æ­£åœ¨è§£æåŸå§‹ API æ•°æ®åŒ… (é›†æˆ Pro ç‰ˆå¤šçº§æå–é€»è¾‘)...")
                        for i, item in enumerate(data_list[:5]):
                            raw = self.extractor.extract_aweme_info(item) or {}
                            r_id = raw.get("aweme_id", "N/A")
                            r_stats = self.extractor.get_item_statistics(raw)
                            r_time = utils.get_time_str_from_unix_time(raw.get("create_time", 0))
                            r_desc = raw.get("desc", "")[:20] + "..."
                            utils.logger.info(f"  #{i+1} ID:{r_id} | èµ:{r_stats['likes']} | è¯„:{r_stats['comments']} | æ—¶é—´:{r_time} | æ–‡æ¡ˆ:{r_desc}")

                    # Handle Verification Case
                    search_nil_info = posts_res.get("search_nil_info", {})
                    if search_nil_info.get("search_nil_type") == "verify_check":
                        utils.logger.warning("ğŸš¨ [SearchHandler] è§¦å‘æŠ–éŸ³å®‰å…¨éªŒè¯ (verify_check)!")
                        if not config.HEADLESS:
                            search_url = f"https://www.douyin.com/search/{keyword}?type=general"
                            utils.logger.info(f"ğŸŒ æ­£åœ¨è·³è½¬è‡³éªŒè¯é¡µé¢ä»¥è§¦å‘æ»‘å—: {search_url}")
                            try:
                                await self.dy_client.playwright_page.goto(search_url)
                                utils.logger.info("â³ è¯·åœ¨æµè§ˆå™¨çª—å£å®ŒæˆéªŒè¯ï¼Œç¨‹åºå°†ç­‰å¾… 60 ç§’...")
                                await asyncio.sleep(60)
                                await self.dy_client.update_cookies(self.dy_client.playwright_page.context)
                                utils.logger.info("âœ… éªŒè¯å®Œæˆï¼Œæ­£åœ¨é‡è¯•å½“å‰é¡µ...")
                                continue
                            except Exception as e:
                                utils.logger.error(f"âŒ è·³è½¬éªŒè¯é¡µé¢å¤±è´¥: {e}")
                                break
                        else:
                            utils.logger.error("âŒ æ— å¤´æ¨¡å¼ä¸‹æ— æ³•æ‰‹åŠ¨éªŒè¯ï¼Œè·³è¿‡æ­¤å…³é”®è¯")
                            # Pro Feature: Update account status to cooldown in DB
                            await self.dy_client.update_account_status("cooldown")
                            break


                    if not data_list:
                        empty_retry_count += 1
                        if empty_retry_count < 3 and has_more:
                            utils.logger.warning(f"âš ï¸ ç¬¬ {page} é¡µ API è¿”å›ä¸ºç©ºï¼Œå³å°†å°è¯•è·³é¡µè¯·æ±‚...")
                            page += 1
                            await asyncio.sleep(config.CRAWLER_TIME_SLEEP)
                            continue
                        else:
                            utils.logger.info(f"ğŸ è¿ç»­å¤šé¡µä¸ºç©ºæˆ–æœè¡¬åˆ°åº•ï¼Œç»“æŸå…³é”®è¯ '{keyword}'")
                            break

                    # Reset empty retry if we found data
                    empty_retry_count = 0
                    
                    # Initialize skip counters
                    skip_stats = {"time": 0, "interaction": 0, "author": 0, "no_vid": 0, "duplicate": 0}
                    aweme_list_to_process = []
                    
                    for item in data_list:
                        # ã€é›†æˆ Pro ç‰ˆä¼˜ç‚¹ã€‘æ”¯æŒå¸¸è§„è§†é¢‘ã€åˆé›†è§†é¢‘ã€å›¾æ–‡ç­‰å¤šçº§è§£æ
                        aweme_info = self.extractor.extract_aweme_info(item)
                        
                        if not aweme_info or not aweme_info.get("aweme_id"): 
                            skip_stats["no_vid"] += 1
                            continue
                            
                        # 0. æ•°æ®åº“æŸ¥é‡ (Pro ç‰ˆç‰¹æ€§)
                        aweme_id = aweme_info.get("aweme_id")
                        if await self.checkpoint_manager.is_note_processed(checkpoint.task_id, aweme_id):
                            skip_stats["duplicate"] += 1
                            continue

                        # --- æœ¬åœ°ç²¾å‡†è¿‡æ»¤é€»è¾‘ ---
                        # ä½¿ç”¨æˆ‘ä»¬åœ¨ config é¢„è®¾å¥½çš„æ—¶é—´æˆ³å’Œé˜ˆå€¼
                        
                        # 1. é—­ç¯æ—¶é—´çª—å£è¿‡æ»¤ [start, end]
                        create_time = aweme_info.get("create_time", 0)
                        if (start_timestamp > 0 and create_time < start_timestamp) or \
                           (end_timestamp > 0 and create_time > end_timestamp):
                            skip_stats["time"] += 1
                            continue
                            
                        # 2. äº’åŠ¨èŒƒå›´è¿‡æ»¤ (Interaction Range)
                        # ä½¿ç”¨ Extractor ç»Ÿä¸€æå–ç»Ÿè®¡æ•°æ®
                        stats = self.extractor.get_item_statistics(aweme_info)
                        likes = stats["likes"]
                        comments_count = stats["comments"]
                        shares = stats["shares"]
                        favorites = stats["favorites"]
                        
                        if (likes < config.MIN_LIKES_COUNT or 
                            comments_count < config.MIN_COMMENTS_COUNT or 
                            shares < config.MIN_SHARES_COUNT or 
                            favorites < config.MIN_FAVORITES_COUNT or
                            (config.MAX_LIKES_COUNT > 0 and likes > config.MAX_LIKES_COUNT) or
                            (config.MAX_COMMENTS_COUNT > 0 and comments_count > config.MAX_COMMENTS_COUNT) or
                            (config.MAX_SHARES_COUNT > 0 and shares > config.MAX_SHARES_COUNT) or
                            (config.MAX_FAVORITES_COUNT > 0 and favorites > config.MAX_FAVORITES_COUNT)):
                            skip_stats["interaction"] += 1
                            continue

                        # 3. åšä¸»å»é‡
                        user_id = aweme_info.get("author", {}).get("uid")
                        if config.DEDUPLICATE_AUTHORS and user_id in processed_authors:
                            skip_stats["author"] += 1
                            continue
                            
                        # 4. èˆ†æƒ…æ•æ„Ÿè¯æœ¬åœ°è¿‡æ»¤ (Sentiment local filter)
                        # å¦‚æœè®¾ç½®äº†èˆ†æƒ…è¯ï¼Œåˆ™æœ¬åœ°å¼ºåˆ¶æ ¡éªŒï¼ˆå³ä¾¿æœç´¢å¬å›äº†ï¼Œä¹Ÿè¦ç¡®ä¿æ–‡æ¡ˆåŒ¹é…ï¼‰
                        if sentiment_keywords:
                            content_text = f"{aweme_info.get('desc', '')} {aweme_info.get('title', '')}".lower()
                            if not any(skw.lower() in content_text for skw in sentiment_keywords):
                                skip_stats["sentiment"] = skip_stats.get("sentiment", 0) + 1
                                continue
                        
                        # å…¨éƒ¨é€šè¿‡è¿‡æ»¤
                        aweme_list_to_process.append(aweme_info)
                        if user_id: processed_authors.add(user_id)
                        
                        if total_processed_count + len(aweme_list_to_process) >= config.CRAWLER_MAX_NOTES_COUNT:
                            break

                    # æ±‡æ€»æ‰“å°è¿‡æ»¤ç»“æœ (Print aggregated skip summary)
                    total_out = len(aweme_list_to_process)
                    utils.logger.info(f"ğŸ“Š ç¬¬ {page} é¡µæ±‡æ€»: APIè¿”å› {total_raw} æ¡ | è¾¾æ ‡ {total_out} æ¡")
                    if total_out == 0 and total_raw > 0:
                        utils.logger.warning(f"  â””â”€ å‰”é™¤åŸå› : æ—¶é—´ {skip_stats['time']} | äº’åŠ¨ {skip_stats['interaction']} | é‡å¤åšä¸» {skip_stats['author']} | èˆ†æƒ…ä¸ç¬¦ {skip_stats.get('sentiment', 0)}")
                    elif total_raw > 0:
                        utils.logger.info(f"  â””â”€ è¿‡æ»¤è¯¦æƒ…: å·²å­˜åœ¨ {skip_stats['duplicate']} | æ—¶é—´ {skip_stats['time']} | äº’åŠ¨ {skip_stats['interaction']} | èˆ†æƒ… {skip_stats.get('sentiment', 0)}")

                    if aweme_list_to_process:
                        await self.aweme_processor.process_aweme_list(aweme_list=aweme_list_to_process, checkpoint=checkpoint)
                        if config.ENABLE_GET_COMMENTS:
                            valid_ids = [a.get("aweme_id") for a in aweme_list_to_process]
                            await self.comment_processor.batch_get_aweme_comments(valid_ids, checkpoint=checkpoint)
                        total_processed_count += len(aweme_list_to_process)

                    # æ›´æ–°è¿›åº¦
                    checkpoint.update_progress(page=page + 1)
                    await self.checkpoint_manager.save(checkpoint)
                    page += 1
                    
                    if total_processed_count >= config.CRAWLER_MAX_NOTES_COUNT:
                        utils.logger.info(f"ğŸ¯ ä»»åŠ¡æŒ‡æ ‡è¾¾æˆï¼å·²æ”¶é½ {total_processed_count} æ¡åˆæ ¼æ•°æ®")
                        break
                    
                    if not has_more:
                        utils.logger.info(f"ğŸ æœç´¢æ± å·²å¹²æ¶¸ï¼Œæ— æ³•è·å–æ›´å¤šç»“æœ")
                        break
                        
                    await asyncio.sleep(config.CRAWLER_TIME_SLEEP)

                except DataFetchError as e:
                    utils.logger.error(f"[SearchHandler] fetch error: {e}")
                    break
                except Exception as e:
                    utils.logger.error(f"[SearchHandler] unexpected error: {e}")
                    break
            
            # Keyword finished
            checkpoint.mark_completed()
            await self.checkpoint_manager.save(checkpoint)
            
            if total_processed_count >= config.CRAWLER_MAX_NOTES_COUNT:
                break
        
        utils.logger.info(f"ğŸ [SearchHandler] ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼Œå…±è®¡æŠ“å–ç¬¦åˆæ¡ä»¶çš„æ•°æ®: {total_processed_count} æ¡")
